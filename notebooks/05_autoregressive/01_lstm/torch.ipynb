{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 200\n",
    "EMBEDDING_DIM = 100\n",
    "N_UNITS = 128\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "LOAD_MODEL = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset\n",
    "with open(\"./full_format_recipes.json\") as json_data:\n",
    "    recipe_data = json.load(json_data)\n",
    "\n",
    "# Filter the dataset\n",
    "filtered_data = [\n",
    "    \"Recipe for \" + x[\"title\"] + \" | \" + \" \".join(x[\"directions\"])\n",
    "    for x in recipe_data\n",
    "    if \"title\" in x\n",
    "    and x[\"title\"] is not None\n",
    "    and \"directions\" in x\n",
    "    and x[\"directions\"] is not None\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the punctuation, to treat them as separate 'words'\n",
    "def pad_punctuation(s):\n",
    "    s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s)\n",
    "    s = re.sub(\" +\", \" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def build_vocab(text_data, vocab_size=VOCAB_SIZE):\n",
    "    word_counts = Counter((word for line in text_data for word in line.split()))\n",
    "    vocab = [\"\", \"[UNK]\"] + [x[0] for x in word_counts.most_common(vocab_size - 2)]\n",
    "    return vocab\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_data, max_len=MAX_LEN, vocab_size=VOCAB_SIZE):\n",
    "        self.text_data = [pad_punctuation(x).lower() for x in text_data]\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = build_vocab(self.text_data, vocab_size=vocab_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_data[idx]\n",
    "        token = torch.tensor(\n",
    "            [\n",
    "                self.vocab.index(x) if x in self.vocab else 1  # [UNK]\n",
    "                for x in text.split(\" \")[: self.max_len]\n",
    "            ]\n",
    "        )\n",
    "        token = F.pad(token, (0, self.max_len - token.shape[0]), value=0)  # [PAD]\n",
    "\n",
    "        x = token[:-1]  # Input sequence\n",
    "        y = token[1:]  # Target sequence\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_units):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, n_units, batch_first=True)\n",
    "        self.dense = nn.Linear(n_units, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        return x  # Output shape: (batch_size, seq_len, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(VOCAB_SIZE, EMBEDDING_DIM, N_UNITS)\n",
    "\n",
    "text_dataset = TextDataset(filtered_data, max_len=MAX_LEN, vocab_size=VOCAB_SIZE)\n",
    "text_dataloader = DataLoader(text_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "optimizer = torch.optim.Adam(lstm.parameters())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    for x, y in text_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = lstm(x)  # Forward pass\n",
    "        loss = loss_fn(out.view(-1, VOCAB_SIZE), y.view(-1))  # Reshape for loss function\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Optimize\n",
    "\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}')  # Log the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 199])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
